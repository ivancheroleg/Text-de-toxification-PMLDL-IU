{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Making Hugging Face dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "268a2edb3ad4678"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "62061863bde7a337"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T14:13:49.185095800Z",
     "start_time": "2023-11-05T14:13:48.702267600Z"
    }
   },
   "id": "b27b0a0c73ad562d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read .tsv file from \"/data/raw folder\"\n",
    "I have also deleted the first column and columns for similarity and length. I will not use them for now."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42031b893312dde0"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-05T14:13:53.892262500Z",
     "start_time": "2023-11-05T14:13:51.837668300Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/filtered.tsv', sep='\\t')\n",
    "\n",
    "# delete unnamed column and columns for similarity and length\n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "df = df.drop(df.columns[2], axis=1)\n",
    "df = df.drop(df.columns[2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sort values pairwise, so less toxic sentences are going first"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "caed20c3d57da8e6"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                                           reference  \\\n0  If Alkar is flooding her with psychic waste, t...   \n1                          Now you're getting nasty.   \n2           Well, we could spare your life, for one.   \n3          Ah! Monkey, you've got to snap out of it.   \n4                   I've got orders to put her down.   \n5  I'm not going to breed kids with a genetic dis...   \n\n                                         translation   ref_tox   trn_tox  \n0  if Alkar floods her with her mental waste, it ...  0.014195  0.981983  \n1                        you're becoming disgusting.  0.065473  0.999039  \n2                      well, we can spare your life.  0.213313  0.985068  \n3                       monkey, you have to wake up.  0.053362  0.994215  \n4                         I have orders to kill her.  0.009402  0.999348  \n5  I'm not gonna have a child... ...with the same...  0.035846  0.950956  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>reference</th>\n      <th>translation</th>\n      <th>ref_tox</th>\n      <th>trn_tox</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>If Alkar is flooding her with psychic waste, t...</td>\n      <td>if Alkar floods her with her mental waste, it ...</td>\n      <td>0.014195</td>\n      <td>0.981983</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Now you're getting nasty.</td>\n      <td>you're becoming disgusting.</td>\n      <td>0.065473</td>\n      <td>0.999039</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Well, we could spare your life, for one.</td>\n      <td>well, we can spare your life.</td>\n      <td>0.213313</td>\n      <td>0.985068</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Ah! Monkey, you've got to snap out of it.</td>\n      <td>monkey, you have to wake up.</td>\n      <td>0.053362</td>\n      <td>0.994215</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I've got orders to put her down.</td>\n      <td>I have orders to kill her.</td>\n      <td>0.009402</td>\n      <td>0.999348</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>I'm not going to breed kids with a genetic dis...</td>\n      <td>I'm not gonna have a child... ...with the same...</td>\n      <td>0.035846</td>\n      <td>0.950956</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df = df.copy()\n",
    "\n",
    "df.loc[temp_df.ref_tox>temp_df.trn_tox, 'reference'] = temp_df.loc[temp_df.ref_tox>temp_df.trn_tox, 'translation']\n",
    "df.loc[temp_df.ref_tox>temp_df.trn_tox, 'translation'] = temp_df.loc[temp_df.ref_tox>temp_df.trn_tox, 'reference']\n",
    "df.loc[temp_df.ref_tox>temp_df.trn_tox, 'trn_tox'] = temp_df.loc[temp_df.ref_tox>temp_df.trn_tox, 'ref_tox']\n",
    "df.loc[temp_df.ref_tox>temp_df.trn_tox, 'ref_tox'] = temp_df.loc[temp_df.ref_tox>temp_df.trn_tox, 'trn_tox']\n",
    "\n",
    "df.head(6)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T14:14:33.487400Z",
     "start_time": "2023-11-05T14:14:33.270613400Z"
    }
   },
   "id": "a638a00aa032ff6d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here we can see that they are now in right order."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c7e4d70e0b2c1d"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True    577777\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print((df['trn_tox'] >= df['ref_tox']).value_counts())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-05T14:14:40.027133700Z",
     "start_time": "2023-11-05T14:14:40.014181300Z"
    }
   },
   "id": "7051b9986d77bd03"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Also, now values of toxicity for reference (non-toxic) sentences are in range [0 : 0.5] and translation (toxic) sentences are in range [0.5 : 1]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e750d7e4087bee05"
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max and min values for reference:\n",
      "max: 0.4994940161705017 ,  min:  3.283871046733111e-05\n",
      "Max and min values for translation:\n",
      "max: 0.9997304081916808 ,  min:  0.5001394152641296\n"
     ]
    }
   ],
   "source": [
    "print(\"Max and min values for reference:\")\n",
    "print(\"max:\", df.ref_tox.max(), \", \", \"min: \", df.ref_tox.min())\n",
    "print(\"Max and min values for translation:\")\n",
    "print(\"max:\", df.trn_tox.max(), \", \", \"min: \", df.trn_tox.min())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T15:29:56.737487300Z",
     "start_time": "2023-10-19T15:29:56.704905800Z"
    }
   },
   "id": "bdd66a78a9664890"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data consists of pairs of non-toxic and toxic sentences along with 4 difference-features.\n",
    "For now I propose to use only the first two columns and label them as \"toxic\" and \"non-toxic\"."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb3f777ea0c8576e"
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "data": {
      "text/plain": "                                           non-toxic  \\\n0  If Alkar is flooding her with psychic waste, t...   \n1                          Now you're getting nasty.   \n2           Well, we could spare your life, for one.   \n3          Ah! Monkey, you've got to snap out of it.   \n4                   I've got orders to put her down.   \n\n                                               toxic  \n0  if Alkar floods her with her mental waste, it ...  \n1                        you're becoming disgusting.  \n2                      well, we can spare your life.  \n3                       monkey, you have to wake up.  \n4                         I have orders to kill her.  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>non-toxic</th>\n      <th>toxic</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>If Alkar is flooding her with psychic waste, t...</td>\n      <td>if Alkar floods her with her mental waste, it ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Now you're getting nasty.</td>\n      <td>you're becoming disgusting.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Well, we could spare your life, for one.</td>\n      <td>well, we can spare your life.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Ah! Monkey, you've got to snap out of it.</td>\n      <td>monkey, you have to wake up.</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I've got orders to put her down.</td>\n      <td>I have orders to kill her.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = df.iloc[:, 0:2]\n",
    "sentences.columns = [\"non-toxic\", \"toxic\"]\n",
    "sentences.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T15:30:21.844556300Z",
     "start_time": "2023-10-19T15:30:21.785084500Z"
    }
   },
   "id": "a4bc4b2e237878fe"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Making dataset\n",
    "To make dataset I will use HuggingFace Datasets library."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f955e119bb93f3bb"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5b24683fc02d8400"
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from datasets import DatasetDict, Dataset, Value"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T15:31:40.902910300Z",
     "start_time": "2023-10-19T15:31:40.899394200Z"
    }
   },
   "id": "57c8c1d8c1a497b3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "I used this [Tutorial](https://kl1p.com/huggingface-dataset-from-pandas-with-code-examples/) to make dataset from pandas dataframe.\n",
    "\n",
    "First we need to define the schema of the dataset. I will use wmt16 dataset structure as in Lab 4 of the PMLDL course, but where \"non-toxic\" sentences are source and \"toxic\" sentences are target. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60ad3ebec7e822f"
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "# Define the schema of the dataset\n",
    "schema = {\n",
    "    \"train\": {\n",
    "        \"translation\" : {\n",
    "            \"non-toxic\": Value(\"string\"),\n",
    "            \"toxic\": Value(\"string\"),\n",
    "        },\n",
    "    },\n",
    "    \"validation\": {\n",
    "        \"translation\" : {\n",
    "            \"non-toxic\": Value(\"string\"),\n",
    "            \"toxic\": Value(\"string\"),\n",
    "        },\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"translation\" : {\n",
    "            \"non-toxic\": Value(\"string\"),\n",
    "            \"toxic\": Value(\"string\"),\n",
    "        },\n",
    "    },\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T15:36:56.332872900Z",
     "start_time": "2023-10-19T15:36:56.328246700Z"
    }
   },
   "id": "31d1f538e90b7a2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "I will divide data into train, validation and test sets. As the dataset is quite big, I will use 90% for training, 5% for validation and 5% for testing."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35d9b2b0ee75e7ff"
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 519999/519999 [00:18<00:00, 28554.97it/s]\n",
      "100%|██████████| 28888/28888 [00:00<00:00, 30084.92it/s]\n",
      "100%|██████████| 28888/28888 [00:00<00:00, 30164.41it/s]\n"
     ]
    }
   ],
   "source": [
    "train_len = int(len(sentences)*0.9)\n",
    "val_len = int(len(sentences)*0.05)\n",
    "test_len = int(len(sentences)*0.05)\n",
    "\n",
    "# To get same structure as in wmt16 dataset, I will use pairwise split.\n",
    "train_pairs = []\n",
    "val_pairs = []\n",
    "test_pairs = []\n",
    "\n",
    "# Create dataset dict\n",
    "dataset = DatasetDict(schema)\n",
    "\n",
    "\n",
    "# Add pairs to lists\n",
    "for i in tqdm(range(train_len)):\n",
    "    train_pairs.append({\"non-toxic\": sentences.iloc[i, 0], \"toxic\": sentences.iloc[i, 1]})\n",
    "    \n",
    "for i in tqdm(range(train_len, train_len+val_len)):\n",
    "    val_pairs.append({\"non-toxic\": sentences.iloc[i, 0], \"toxic\": sentences.iloc[i, 1]})\n",
    "    \n",
    "for i in tqdm(range(train_len+val_len, train_len+val_len+test_len)):\n",
    "    test_pairs.append({\"non-toxic\": sentences.iloc[i, 0], \"toxic\": sentences.iloc[i, 1]})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T15:37:18.819447300Z",
     "start_time": "2023-10-19T15:36:58.662326700Z"
    }
   },
   "id": "d036480752a16a9f"
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [],
   "source": [
    "# Add pairs through dataset dict\n",
    "dataset[\"train\"] = Dataset.from_dict({\"translation\": train_pairs})\n",
    "dataset[\"validation\"] = Dataset.from_dict({\"translation\": val_pairs})\n",
    "dataset[\"test\"] = Dataset.from_dict({\"translation\": test_pairs})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T15:37:20.437428300Z",
     "start_time": "2023-10-19T15:37:20.348306500Z"
    }
   },
   "id": "ee0e17b32a9eff31"
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "data": {
      "text/plain": "{'translation': [{'non-toxic': 'If Alkar is flooding her with psychic waste, that explains the high level of neurotransmitters.',\n   'toxic': 'if Alkar floods her with her mental waste, it would explain the high levels of neurotransmitter.'},\n  {'non-toxic': \"Now you're getting nasty.\",\n   'toxic': \"you're becoming disgusting.\"}]}"
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][:2]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T15:37:21.144266100Z",
     "start_time": "2023-10-19T15:37:21.134205900Z"
    }
   },
   "id": "fae8761fb7075568"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save dataset to file .\\data\\interim\\justification dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fc24e6d315aee286"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6c2ac1d6ab8885d4"
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/519999 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9eec79415d3f44f0b4a8b4afccbf93ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/28888 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98feb2c921054e5c8d9cd3281d2e2749"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Saving the dataset (0/1 shards):   0%|          | 0/28888 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "767ebb14783146a7a71a736c404250d5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(\"../data/interim/justification_dataset\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-19T15:39:28.393172700Z",
     "start_time": "2023-10-19T15:39:28.111186800Z"
    }
   },
   "id": "49c9351e9713a5f5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note:** This python notebook is only demonstration of the data preprocessing and my way to make the dataset. The script for making dataset is in the folder \"src/data\" and is called \"make_dataset.py\". Instructions for running it are in the README.md file."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "480a2b60869f0e86"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
